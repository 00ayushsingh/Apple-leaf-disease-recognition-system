# -*- coding: utf-8 -*-
"""G3MINOR5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zd0jhpAxtW0M_jva5fbzee-LePAOHLKF
"""

from IPython.display import clear_output
!pip install vit_keras
clear_output()

!pip install tensorflow-addons

import os
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import cv2
from vit_keras import vit

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score, f1_score,RocCurveDisplay
from sklearn.decomposition import PCA
from sklearn.svm import SVC

import tensorflow as tf
from tensorflow.keras.layers import Dense, Input, Flatten
from tensorflow.keras.models import Model

IMG_SIZE = 224, 224
BATCH_SIZE = 32
SEED = 1
AUTO = tf.data.AUTOTUNE

# Creatin mirrored strategy and the model

stg = tf.distribute.MirroredStrategy()

# Define the function to create a list of image paths
def create_images_list(path):
    full_path = []
    images = sorted(os.listdir(path))
    for i in images:
        full_path.append(os.path.join(path, i))

    return full_path

# Update paths to Google Drive folders
train_healthy = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/train/healthy')
train_rust = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/train/rust')
train_scab = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/train/scab')

val_healthy = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/val/healthy')
val_rust = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/val/rust')
val_scab = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/val/scab')

test_healthy = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/test/healthy')
test_rust = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/test/rust')
test_scab = create_images_list('/content/drive/MyDrive/appledata_augumented_splitted/test/scab')

# Lookup table
leaf_disease_classes = {0:'healthy', 1 : 'rust', 2 : 'scab'}

# Create dataframes
train_data = pd.concat([pd.DataFrame({'img': np.array(train_healthy), 'label':0 }),
                        pd.DataFrame({'img': np.array(train_rust)[20:], 'label':1 }),
                        pd.DataFrame({'img': np.array(train_scab)[20:], 'label':2 }) ], ignore_index = True)

val_data = pd.concat([pd.DataFrame({'img': np.array(val_healthy), 'label':0 }),
                      pd.DataFrame({'img': np.array(val_rust), 'label':1 }),
                      pd.DataFrame({'img': np.array(val_scab), 'label':2 }) ], ignore_index = True)

test_data = pd.concat([pd.DataFrame({'img': np.array(test_healthy), 'label':0 }),
                       pd.DataFrame({'img': np.array(test_rust), 'label':1 }),
                       pd.DataFrame({'img': np.array(test_scab), 'label':2 }) ], ignore_index = True)

# Shuffle the data
train_data = shuffle(train_data, random_state = SEED).reset_index(drop = True)
val_data = shuffle(val_data, random_state = SEED).reset_index(drop = True)
test_data = shuffle(test_data, random_state = SEED).reset_index(drop = True)

# There is a wrong file extend, find and delete

wrong_idx = 0
for i in range(train_data.shape[0]):
    if train_data['img'].values[i][-3:] == 'jpg':
        pass
    else:
        wrong_idx = i
        print('wrong file -> ',train_data['img'].values[i])


# Dropping wrong file
train_data.drop([wrong_idx], axis = 0, inplace = True)


print("train shape -> ", train_data.shape[0])
print("val shape -> ", val_data.shape[0])
print("test shape -> ", test_data.shape[0])

# Reading -> Resizing -> Normalization
def img_preprocessing(image, label):
    img = tf.io.read_file(image)
    img = tf.io.decode_jpeg(img, channels = 3)
    img = tf.image.resize(img, size = (IMG_SIZE))
    img = tf.cast(img, tf.float32) / 255.0

    return img, label


# Data augmentation
def augmentation(image, label):
    img = tf.image.random_flip_left_right(image, seed = SEED)
    img = tf.image.random_flip_up_down(img, seed = SEED)
    img = tf.image.random_brightness(img, 0.1, seed = SEED)
    img = tf.image.random_contrast(img, 0.2, 0.4, seed = SEED)
    img = tf.image.random_saturation(img, 2, 6, seed = SEED)

    return img, label

# Creating dataset loaders and tf.datasets

train_loader = tf.data.Dataset.from_tensor_slices((train_data['img'], train_data['label']))
train_dataset = (train_loader
                 .map(img_preprocessing, num_parallel_calls = AUTO)
                 .map(augmentation, num_parallel_calls = AUTO)
                 .shuffle(BATCH_SIZE * 10)
                 .batch(BATCH_SIZE)
                 .prefetch(AUTO))


# Training dataset without shuffling and data augmantation operations for the classification stage
train_loader_feature = tf.data.Dataset.from_tensor_slices((train_data['img'], train_data['label']))
train_dataset_feature = (train_loader_feature
                         .map(img_preprocessing, num_parallel_calls = AUTO)
                         .batch(BATCH_SIZE)
                         .prefetch(AUTO))


valid_loader = tf.data.Dataset.from_tensor_slices((val_data['img'], val_data['label']))
valid_dataset = (valid_loader
                 .map(img_preprocessing, num_parallel_calls = AUTO)
                 .batch(BATCH_SIZE)
                 .prefetch(AUTO))


test_loader = tf.data.Dataset.from_tensor_slices((test_data['img'], test_data['label']))
test_dataset = (test_loader
                 .map(img_preprocessing, num_parallel_calls = AUTO)
                 .batch(BATCH_SIZE)
                 .prefetch(AUTO))

from vit_keras import vit

with stg.scope():
    vit_model = vit.vit_b16( image_size = IMG_SIZE, activation = 'softmax', pretrained = True, include_top = False, pretrained_top = False, classes = 3)

    inp = Input(shape = (*IMG_SIZE, 3))
    vit = vit_model(inp)
    X = Flatten()(vit)
    X = Dense(64, activation = 'gelu', name = 'the_feature_layer')(X)
    X = Dense(32, activation = 'gelu')(X)
    out = Dense(3, activation = 'softmax')(X)

    model = Model(inputs = inp, outputs = out)
    model.summary()

    model.compile(optimizer = tf.keras.optimizers.AdamW(learning_rate = 0.0001,weight_decay = 0.0001),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics = ['acc',tf.keras.metrics.SparseTopKCategoricalAccuracy(k = 2, name = "top_2_acc", dtype=None) ] )

# Training feature extraction model and saved

hist = model.fit(train_dataset, epochs = 5, batch_size = 16, validation_data = valid_dataset)
model.save("vit_feature_extractor.h5")

# Validation and Test evaluations of ViT model

with stg.scope():
    print('ViT model results')
    print('--'*50)
    val_eval_vit = model.evaluate(valid_dataset)
    print('Validation Loss: {0:.3f}'.format(val_eval_vit[0]))
    print('Validation Accuracy: {0:.3f} %'.format(val_eval_vit[1]*100))
    print('--'*50)
    test_eval_vit = model.evaluate(test_dataset)
    print('Test Loss: {0:.3f}'.format(test_eval_vit[0]))
    print('Test Accuracy: {0:.3f} %'.format(test_eval_vit[1]*100))

# Reading saved model and weights
feature_extr = tf.keras.models.load_model('/content/vit_feature_extractor.h5')

# Feature extraction model
feature_extractor_model = Model(inputs=feature_extr.input,
                                outputs=feature_extr.get_layer('the_feature_layer').output)

# Creating train features

with stg.scope():
    features = feature_extractor_model.predict(train_dataset_feature)

# Applying PCA 41 components nearly equal to 0.98 variance ratio

pca_ = PCA(41)
pred_pca_ = pca_.fit(features)
pred_pca = pred_pca_.transform(features)

new_feature_column_names = []
for i in range(pred_pca.shape[1]):
    new_feature_column_names.append('feature_{0}'.format(i+1))

train_features = pd.DataFrame(pred_pca, columns = new_feature_column_names)


# Features created with a pretrained ViT feature extractor
train_features.head()

clf = SVC(kernel = 'rbf', random_state = SEED)

# Training SVM model with features from the ViT feature extractor
clf.fit(train_features,  train_data['label'])

# Test set processing
# test feature exteaction -> applying PCA -> predictions of the LogisticRegression model

with stg.scope():
     test_features = feature_extractor_model.predict(test_dataset)

test_features = pca_.transform(test_features)
test_features = pd.DataFrame(test_features, columns = new_feature_column_names)


test_pred = clf.predict(test_features)

# Predictions and scores

mse = mean_squared_error(test_data['label'], test_pred)
f1 = f1_score(test_data['label'], test_pred, average = 'weighted')
acc = accuracy_score(test_data['label'], test_pred)

print('Mean Squared Error : {0:.5f}'.format(mse))
print('Weighted F1 Score : {0:.3f}'.format(f1))
print('Accuracy Score : {0:.3f} %'.format(acc*100))

# classification report

clf_report = classification_report(test_data['label'], test_pred, target_names = list(leaf_disease_classes.values()))
print(clf_report)

cm = confusion_matrix(test_data['label'], test_pred)
cmd = ConfusionMatrixDisplay(cm, display_labels = list(leaf_disease_classes.values()))

fig, ax = plt.subplots(figsize=(5, 5))
cmd.plot(ax=ax, colorbar = False)

test_take1 =  test_dataset.take(-1)
test_take1_ = list(test_take1)

# A function that creating 5 random images in the test set and predictions

# Red title -> a false prediction
# Green title -> a true prediction

def random_test_sample_with_prediction(SEED):
    idxs = np.random.default_rng(seed=SEED).permutation(len(test_pred))[:5]
    batch_idx = idxs // BATCH_SIZE
    image_idx = idxs-batch_idx * BATCH_SIZE
    idx = idxs

    fig, axs = plt.subplots(1,5, figsize = (12,12) ,dpi = 150)

    for i in range(5):
        img = test_take1_[batch_idx[i]][0][image_idx[i]]
        label = test_take1_[batch_idx[i]][1][image_idx[i]].numpy()


        if int(test_pred[idx[i]]) == label:
            axs[i].imshow(img, cmap = 'gray')
            axs[i].axis('off')
            axs[i].set_title('image (no: ' + str(idx[i])  + ')' + '\n' + leaf_disease_classes[label], fontsize = 8, color = 'green')
        else:
            axs[i].imshow(img,  cmap = 'gray')
            axs[i].axis('off')
            axs[i].set_title('image (no: ' + str(idx[i])  + ')' + '\n' + leaf_disease_classes[label], fontsize = 8, color = 'red')

# Red title -> a false prediction
# Green title -> a true prediction

random_test_sample_with_prediction(SEED = 10)
random_test_sample_with_prediction(SEED = 43)
random_test_sample_with_prediction(SEED = 999)

